---
title: 理解线性代数
date: 2017-07-29 19:03:05
categories: style
tags: ['linear algebra', 'math']
mathjax: true
---

> <center>序 碎碎念</center>
> 好吧，这八成又是一篇没有结尾的碎碎念。
>
> 从三月到现在，我一直在琢磨着，线性代数那抽象的符号下有啥具体的含义呢？我不想要抽象，我只想要在某个具体场景下的指代。现在呢？看过[《线性代数的本质》](#the-essence-of-linear-algebra)后，很是惬意，自认为找到了它在几何上的解释。
>
> 这周扫过一遍那脏兮兮的教材，我把自己的疑问分成如下几个点：
>
> * 矩阵乘法
> * 矩阵的逆
> * 伴随矩阵
> * 行列式与矩阵的关系
> * 矩阵的秩
> * 矩阵的特征值与特征向量
> * 相似矩阵
> * 正交矩阵
> * 合同矩阵
> * 向量空间
> * 向量与线性方程组之间的关系
> * 向量组与矩阵的关系
> * 向量组的线性组合、线性相关是什么
>
> 以上，都是些什么东西？这是我一直想弄明白的问题。
>
> 我觉得，人家用两个多小时的视频动画来解释上述东西的几何意义。我呢？只能用狗屎一般的文字来把自己的理解叙述出来。当然不可能如人家讲得明白啦，也没期望有缘人可以从我这里弄明白这些东西，有恍然大悟的感觉。但可以当个延伸吧，毕竟，我也探究了好几个月，看过的解说也不少（但认为系列的视频最简单易懂啦），也会有点视频之外的收获，不是么？

## 从《上帝掷骰子吗》到向量空间

不知道你是否看过[《上帝掷骰子吗》](https://book.douban.com/subject/1467022/)这本书，如果看过的话，那薛定谔的猫是死了还是活着呢？这是一个很有意思的问题：猫未来的状态是不确定的，但穿越到未来的那个时间点，猫的状态就是确定的（未来没有如果）。书中提出了多维宇宙与平行宇宙的概念。这让我想到《三体 死神永生》中所描述的四维宇宙：当前的三维空间只是四维空间中的一个投影罢了。同样，不同的宇宙只是高纬宇宙中的不同投影罢了；未来本身就摆在哪里，只是当前的选择使自己步入了不同的宇宙。

那么，线性代数中的向量空间是什么呢？在几何上，一个方向与实数集中的点搭配，组成了一个数轴，也就是一条线，它是一维的；两条不平行的直线确定一个面，也就是说，两条线可拓展成一个二维空间；再来一个方向的线呢，就成了三维空间。这就是向量空间的概念。列就表示一个向量。

## 隐藏在图形学中的矩阵乘法

坐标系的原点不同，单位向量不同，即使是描述同一条直线，向量的表示形式也会不同。那么，如果在 $\alpha$ 坐标系下直线的表示形式是 $\vec{v}$ ，怎么知道在 $\beta$ 坐标系下的表示形式 $\vec{w}$ 呢？这就用到了矩阵乘法：假如知道 $\boldsymbol{ \alpha }$ 坐标系与 $\boldsymbol{ \beta }$ 坐标系存在某种关系： $\boldsymbol { \beta = A \alpha }$ ，那么就会有 $\vec{w} = \boldsymbol{ A } \vec{v}$ 。

在计算机图形学中，几何图形的变换分为缩放、旋转、切变与平移。我认为缩放是最简单的一种变换。对二维空间来说，$x$ 、$y$ 轴分别缩放 $m$、$n$ 倍，有着如下的变换矩阵：{% raw %}$
\boldsymbol{ A } =
\begin{bmatrix}
m & 0 \\
0 & n \\
\end{bmatrix}
${% endraw %} 。应该很容易想到它的几何变换，无非就是伸缩坐标轴。

![zoom](/images/17/07/zoom.png)

那么旋转呢？很容易搜索到二维空间的旋转矩阵：逆时针旋转 $\alpha$，{% raw %}$
\boldsymbol{ M(\alpha) } =
\begin{bmatrix}
\cos \alpha & -\sin \alpha \\
\sin \alpha & \cos \alpha
\end{bmatrix}
${% endraw %}。暂且忽略它的数值，而关心它是怎么得到的：以坐标系的原点为圆心，把它旋转 $\alpha$ ，使用旋转前的坐标系表示当前单位向量，$x$ 轴是 {% raw %}$
\begin{bmatrix}
\cos \alpha \\
\sin \alpha
\end{bmatrix}
${% endraw %}，$y$ 轴则是 {% raw %}$
\begin{bmatrix}
-\sin \alpha \\
\cos \alpha
\end{bmatrix}
${% endraw %}。比如旋转 $\frac{\pi}{2}$：

![rotation](/images/17/07/rotate.png)

对于切变呢？比如：{% raw %}$
\begin{bmatrix}
3 & 1 \\
0 & 2
\end{bmatrix}
${% endraw %}：

![shear](/images/17/07/shear.png)

平移比较有意思。对平移来说，二维向量 {% raw %}$
\begin{bmatrix}
v_1 & w_1 \\
v_2 & w_2
\end{bmatrix}
${% endraw %}，需要先增加一维变成 {% raw %}$
\begin{bmatrix}
v_1 & w_1 & 0 \\
v_2 & w_2 & 0 \\
0   & 0   & 1
\end{bmatrix}
${% endraw %}，之后再借助三维空间的变换矩阵(沿着 $x$ 轴平移 $x_1$、$y$ 轴平移 $y_1$ 个单位长度)：{% raw %}$
\begin{bmatrix}
1 & 0 & x_1 \\
0 & 1 & y_1 \\
0 & 0 & 1
\end{bmatrix}
${% endraw %}。其实想说明的是：平移需要借助更高一级的维度来完成。

## 矩阵的秩与多维空间

思考一个问题：为什么平移的变换矩阵需要使用额外的一维呢？我们知道，在不同的坐标系下描述同一个东西，矩阵乘法提供了一种可行的方法，它所进行的变换是线性的，有两个特点：1. 变换前后的坐标原点不改变；2. 变换后网格线平行并且等距的特点仍然保持。平移改变了坐标系的原点，当前线性空间下它所进行的不是线性变换，但在更高一级的维度上，它只是一种切变。比如一维空间，在二维空间上进行切变，向量从一条线被平移到了另一条线上。矩阵的逆则是一种逆变换，对于向量 $\vec {v}$，$\boldsymbol {BA} \vec{v}$ 的含义是先进行 $\boldsymbol {A}$ 变换，再进行 $\boldsymbol {B}$ 变换，要变换回去呢，则先要进行 $\boldsymbol {B^{-1}}$ 变换，再进行 $\boldsymbol {A^{-1}}$ 咯。这样，是不是很容易理解 $\boldsymbol {(AB)^{-1} = B^{-1}A^{-1}}$ 为什么长成这样了？

那么，经过线性变换后，所张成的向量空间还是原来的那个空间吗？这就与矩阵的秩有关了。秩代表线性变换后空间的维数。对于满秩矩阵来说，空间依旧；非满秩矩阵就不一定了，高维空间会被压缩到低维。二维空间可以压缩为一维空间、零维空间；三维呢，则是二维、一维与零维。具体是哪一个，是根据变换矩阵的秩来决定的。

上述的零空间，称为零向量空间更确切一些。这才是零空间的真正含义：它是向量空间，是齐次线性方程组 $\boldsymbol {A} \vec{v} = \vec{0}$ 的所有解 $\vec{v}$ 组成的集合。这时候矩阵 $\boldsymbol {A}$ 必然不是满秩矩阵，空间被压到了低维。此时，向量、矩阵与线性方程组的关系应该很明确了：线性方程组的系数构成系数矩阵，如果是非齐次线性方程组，需要写成增广矩阵的形式。系数矩阵可以求出解空间，而增广矩阵，代表了解空间里面一种特殊的解。

既然存在降维的线性变换，那么能否升维吗？这就是左乘一个非方阵的一个含义。比如 {% raw %}$\boldsymbol {A _ {3 * 2}} ${% endraw %}，它代表着将二维空间映射到三维上，多出来的一维怎么定义，是根据矩阵 $\boldsymbol {A}$ 来决定的。也有非方阵 {% raw %}$\boldsymbol {A _ {2 * 3}}${% endraw %}，是将三维映射至二维，不过我觉得与非满秩的方阵比起来，它就少了点零而已。总之，可以把这东西认为是一种映射，输入二维输出三维，输入三维输出二维，仅此而已。

## 空间的缩放——行列式的值

再回到秩这个定义上，我们都知道每一个矩阵都对应着一个行列值 $\det (\boldsymbol {A})$，那么矩阵的行列式究竟表达了一种怎样的含义？

![determinate negative](/images/17/07/determinant-negative.png)
![determinant zero](/images/17/07/determinant-zero.png)

直截了当地说：行列式测量了一个给定区域增大或减小的比例。比如二维空间，线性变换会影响区域面积的大小，如果值为零，则代表了这个矩阵所进行的变换是将空间压缩到更小的维度上。至于行列式的符号，指的是空间是否发生反转，简单地说，就是 $x$ 轴原本位于 $y$ 轴的逆时针方向，经过变换后位于 $y$ 轴的顺时针方向了。$\det(M_1 M_2) = \det(M_1) \det(M_2)$ 是显然易见的：两次变换后扩大的倍数等于两次变换分别扩大的倍数之积。

继续之前，说一下基。它可以帮助理清线性组合和线性相关、线性无关这些概念。一个 $n$ 维向量空间，必然由 $n$ 个互不平行的向量张成，这个向量组可以称为这个向量空间的一个基。这些向量是线性无关的；若向量变得更多，他们就是线性相关的了。$n$ 维向量空间中的任意一个向量 $\vec{v}$ 都可以由基表示而成，称 $\vec {v}$ 是这个向量组的线性组合。

## 矩阵透露出的特征

那么该如何理解式子 $\boldsymbol A \vec{v} = \lambda \vec{v}$ 中的特征值 $\lambda$ 与特征向量 $\boldsymbol{A}$？要明确的是：向量 $\boldsymbol {A}$ 具有两个属性：值与方向，就好象速度有大小和方向一样，矩阵 $\boldsymbol {A}$ 左乘一个向量 $\vec {v}$ 代表的是一个线性变换。经过线性变换后，$\vec {v}$ 会张满整个向量空间。特殊情况下，$\vec {v}$ 在空间中仅被拉伸和压缩，$\lambda \vec {v}$ 代表的便是这种特殊的情形。简单地说：特征值对应拉伸的大小，特征向量对应拉伸的方向。

![eigenvalues and eigenvectors](/images/17/07/eigenvalues-and-eigenvectors.png)

一个方阵 $\boldsymbol {A}$ 会具有不同的特征值，也可能包含一些 $k$ 重特征值，每一个(重)特征值都对应着一个特征向量。特征值的绝对值则代表矩阵 $\boldsymbol {A}$ 在每个基上的投影长度，它越大，说明矩阵在对应的特征向量上的变化越快，“特征”更明显。这个特性在图像压缩与数据处理中有很好的作用，它提供了一种降维处理数据的方法。来源：[如何理解矩阵特征值？](https://www.zhihu.com/question/21874816/answer/19592526)，参考[主成成分分析](https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)。

## 相似矩阵：同一种事物的不同语言描述

书中提到一个定理：相似矩阵有着相同的特征多项式，从而有着相同的特征值。这究竟说的是啥？相似矩阵又是什么东西？先思考这个现象：俩坐标系 $\alpha$ 与 $\beta$，他们的关系是 $\beta = \boldsymbol {P} \alpha$，那么在 $\beta$ 中进行线性变换(矩阵 $\boldsymbol {A}$)，怎么在坐标系 $\alpha$ 中表示？下面的动画描述了这么个场景：Jennifer 的语言可以描述 $\alpha$ 坐标系下的向量，在 $\beta$ 坐标系中发生了旋转，要用 Jennifer 的语言描述这个旋转：

![how-to-translate-a-matrix](/images/17/07/how-to-translate-a-matrix.png)

以上，就是相似矩阵 $\boldsymbol {P^{-1}AP} = \boldsymbol {B}$ 的含义。至于那个定理，暗示着：在不同坐标系下进行相同的变换，他们有着共同的特征。已经知道，任何一个线性变换是由基本的线性变换(缩放、旋转、切变)组成的。如果一个变换矩阵 $\boldsymbol {M}$ 能够分解为 $\boldsymbol {P^{-1} \Lambda P}$，那简直太美妙了，$\boldsymbol {M^{n}}$ 的计算一定飞起(可对角化的矩阵)。

## 点积、叉积与二次型

高中的时候，我们就学过点积与叉积。点积那时候叫做数量积，至少 $\vec{a} \cdot \vec{b} = |\vec{a}||\vec{b}| \cos \theta$ 记得吧。没啥好说的，两个向量点乘，就是代表了一种变换，变换成一个数。两个向量之间的叉积是一个向量，它的值是有着某种意义的：二维向量空间是面积，三维向量空间是体积，正负值代表一种方向。从线性变换的角度看，它是一种高维空间到一维空间的线性变换，能够使得应用线性变换后的结果与一个向量的点乘等价，当然有方向啦。在我看来，他俩其实就代表了一种变换：点积是高维到数，叉积是高维到一维。不同空间下对同一种事物的描述(多对一的关系呗)。
{% raw %}
$$
\begin{bmatrix}
p_1 \\
p_2 \\
p_3
\end{bmatrix}
\cdot
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
=
\det {
\begin{bmatrix}
x & v_1 & w_1 \\
y & v_2 & w_2 \\
z & v_3 & w_3
\end{bmatrix}
}
$$

$$
p_1 \cdot x + p_2 \cdot y + p_3 \cdot z =
\begin{matrix}
x(v_2 \cdot w_3 & - & v_3 \cdot w_2) & +  \\
z(v_3 \cdot w_1 & - & v_1 \cdot w_3) & +  \\
z(v_1 \cdot w_2 & - & v_2 \cdot w_1) &
\end{matrix}
$$
{% endraw %}

在点积的基础上理解二次型要容易得多。二次型 {% raw %}$f(x_1, x_2, ..., x_n)$ {% endraw %}的矩阵形式为 {% raw %}$f(\boldsymbol{x}) = \boldsymbol {x^T A x}${% raw %}。它就代表了一种多维空间到数的变换。两个矩阵 $\boldsymbol {A, B} $合同($\boldsymbol {A} \backsimeq \boldsymbol {B}$)定义为 $\boldsymbol {C^T A C} = \boldsymbol {B}$，它很像对两个向量求点积 $\boldsymbol {C^T A C} = \boldsymbol {C^T} \cdot \boldsymbol {M}$。对二次型进行合同变换的本质就是换一组坐标系重新看待这个二次型，反正都是描述同一个东西，不是吗？

## 什么是线性代数？

把线性代数中的概念依附于某一个具体的场景，并不具有普适性。就像物理学家们追求解释万物的语言一样，数学家们也在追求适用于万物的数学定理：从不同的场景中抽象出共性，罗列一清单，他们并不特指某一具体的场景，而是让使用者们在特定的场景下验证这些条条框框(定理)，只要这些定义被满足，他们就会有那样的性质。

抽象出来的概念固然难以理解，个人认为人类的认知依附于具体的事物上才更容易，然后举一反三，更好地利用已经理解到的东西。人工智能距离这一天，会远吗？

## 推荐阅读 && 参考资料

[1] [《线性代数的本质》](https://www.bilibili.com/video/av6731067/)  
[2] [《上帝掷色子吗》](https://book.douban.com/subject/6434486/)  
[3] [知乎: 如何理解线性代数？](https://www.zhihu.com/question/20534668)  
[4] [知乎: 如何理解矩阵特征值？](https://www.zhihu.com/question/21874816)  
[5] [知乎: 如何形象的理解矩阵的相似与合同？](https://www.zhihu.com/question/21931863)  
[6] [知乎: 二次型的意义是什么？有什么应用？](https://www.zhihu.com/question/38902714)  
[7] [知乎专栏: 向量、矩阵、三维变换及四元数](https://zhuanlan.zhihu.com/p/22578991)  
[8] [维基百科: 变换矩阵](https://zh.wikipedia.org/zh-hans/%E5%8F%98%E6%8D%A2%E7%9F%A9%E9%98%B5)  
[9] [维基百科: 零空间](https://zh.wikipedia.org/wiki/%E9%9B%B6%E7%A9%BA%E9%97%B4)  

<div font-family="monospace important!">- 08/01/2017 00:06 Beijing Chaoyang</div>
